---
title: "data_comp"
author: "Shalini Rallapalli Venkata"
date: "April 23, 2019"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r}
data_train<-read.csv('C:/Users/shali/Desktop/IST 5535/train_data.csv')
data_test<-read.csv('C:/Users/shali/Desktop/IST 5535/test_data.csv')


```

```{r}
#Variable selection
str(data_train)
data_train$class<-factor(data_train$class)
str(data_train)
data_test$class<-factor(data_test$class)


```

```{r}
plot(class~word_freq_make , data = data_train, col="green", main ='Boxplot 1')
```
The frequency of the word 'make', is highest between 0 and 0.5 with its class being 0. Thus, the make word belongs to not spam class.
```{r}
plot(class~word_freq_address   , data = data_train, col="pink", main ='Boxplot 1')

```
The frequency of the word 'adress', is highest between 0 and 1 with its class being 0. Thus, the make word belongs to not spam class.
```{r}
hist(data_train$capital_run_length_average, 
     main="Histogram for capital_run_length_average", 
     xlab="capital rum length avg", 
     border="pink", 
     col="green",
     xlim=c(10,1200),
     las=1, 
     breaks=5)
```
The capital run length on an average lies between 0 to 200.

```{r}
hist(data_train$capital_run_length_total, 
     main="Histogram for capital_run_length_average total", 
     xlab="capital rum length avg", 
     border="blue", 
     col="red",
     breaks=5)
```

The total capital run length on an average lies between 0 to 2000.

```{r}
#Let us consider the frequency of few words and plot it with the target spam/not spam variable
pairs(class~word_freq_all + word_freq_mail + word_freq_addresses + capital_run_length_average + 
        capital_run_length_longest + capital_run_length_total , data= data_train)
```
```{r}
library(MASS)
lda.fit.comp=lda(class~., data = data_train)
lda.fit.comp
plot(lda.fit.comp)
```

```{r}
lda.pred.comp=predict(lda.fit.comp)
```

```{r}
#Evaluation on test data set
library(MASS)
lda.fit.comp.test<-predict(lda.fit.comp, data_test)

```

```{r}
names(lda.fit.comp.test)
```


```{r}
library(caret)
confusionMatrix(lda.fit.comp.test$class,data_test$class, positive = "1")
```


TP= 417
TN= 793
FP=127
FN= 44
Precision = TP/TP+FP =417/544 =0.766, Recall= TP/TP+FN = 417/461=0.904, F1 score of model with training data set= 2* precision*recall/ precision+recall =2*(0.692464/1.673) = 0.827 i.e: 82.7%

F1 score for lda is 87%

Lets consider other models


```{r}
library(randomForest)
bag_class<-randomForest(class~., data = data_train,mtry = 57, importance = TRUE, positive="1")
bag_class
```


```{r}
#Test the performance of the bagging tree on the test dataset.
library(caret)
bag_yhat <-predict(bag_class, newdata = data_test)
confusionMatrix(bag_yhat,data_test$class, positive="1")

```
```{r}
precision <- 500/(544)
recall <- 500/(536)

F1 <- (2 * precision * recall) / (precision + recall)
F1
```

Let us calculate the performance of the tree when all the predictors are considered mtry=57. 
TP= 502
TN= 803
FP=42
FN= 34
Precision = TP/TP+FP =502/544 =0.922, Recall= TP/TP+FN = 502/536=0.936, F1 score of model with training data set= 2* precision*recall/ precision+recall =2*(0.862992/1.858) = 0.928 i.e: 92.5%



Let us consider SVM
```{r}
library(e1071)
svm_class<-svm(class~., data = data_train,kernel ='sigmoid', cost = 100, scale = TRUE)
summary(svm_class)
```
```{r}
svm_yhat <-predict(svm_class, newdata = data_test)
```
Since the accuracy is less lets tune the parameters
```{r}
set.seed(1)
tune_svm <-tune(svm, class~., data = data_train,kernel ='sigmoid',ranges =list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
```

```{r}
svm_yhat2 <-predict(tune_svm$best.model, newdata = data_test)
```
Accuracy has increased lets consider the F1-score
```{r}
confusionMatrix(svm_yhat2,data_test$class, positive="1")
```

Let us calculate the performance of svm with the best tuned parameter 
TP= 430
TN= 789
FP=114
FN= 48
Precision = TP/TP+FP =430/544 =0.79, Recall= TP/TP+FN = 430/478=0.899,Accuracy= 1219/1381= 0.882 ~88.2%. F1 score of model with training data set= 2* precision*recall/ precision+recall =2*(0.71021/1.689) = 0.84 i.e: 84%

Now let us consider ANN

```{r}
library('neuralnet')
library(caret)
# calculate the pre-process parameters from the training dataset
preprocessParams <-preProcess(data_train, method =c("range"))
# summarize transform parameters
print(preprocessParams)
train_scaled <-predict(preprocessParams, data_train)
# Fit a neural network model with 2 hidden layers
nn_fit<-neuralnet(class~. , data = train_scaled, hidden =c(4,2), linear.output=FALSE)
summary(nn_fit)
#plot(nn_fit)
```



```{r}
test_scaled <-predict(preprocessParams, data_test)
pred_norm <-compute(nn_fit, test_scaled)
pred <- pred_norm$net.result
length(pred)
pred_final <- ifelse(pred>0.5, 1, 0)
confusionMatrix(factor(pred_final[,2]),factor(data_test$class), positive="1")
```

```{r}


precision <- 501/(544)
recall <- 501/(501+44)

F1 <- (2 * precision * recall) / (precision + recall)
F1
```

F1-score is 92%

```{r}
library(keras)
data_size <- 1000
model <- keras_model_sequential()
model %>% 
  layer_embedding(input_dim = data_size, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% summary()
```






```{r}
model %>% compile(
  optimizer = 'adam',
  loss = 'binary_crossentropy',
  metrics = 'accuracy'
)
```


```{r}
library(dplyr)
X <- data_train %>% 
  select(-class) %>% 
  scale()
X<-as.array(X)
y<- data_train$class

```

```{r}
library(dplyr)
X_test <- data_test %>% 
  select(-class) %>% 
  scale()
X_test<-as.array(X_test)
y_test<- data_test$class
```



```{r}
history <- model %>% fit(
  as.array(X),
  y,
  epochs = 40,
  batch_size = 256,
  validation_split = 0.2
)
```

```{r error=TRUE}
library(ggplot2)
plot(history)
```

```{r}
results <- model%>% evaluate(X_test, y_test)
results
```
Model accuracy is very less.


